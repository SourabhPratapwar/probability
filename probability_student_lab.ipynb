{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {"provenance": []},
    "kernelspec": {"name": "python3", "display_name": "Python 3"},
    "language_info": {"name": "python"}
  },
  "cells": [
    {"cell_type":"markdown","metadata":{},"source":[
      "# Probability Essentials — FAANG-Level Lab\n",
      "\n",
      "**Goal:** ML-relevant probability: expectation, variance, Bayes, and simulation checks.\n",
      "\n",
      "**Outcome:** You can reason about uncertainty, distributions, and Bayes updates (interview-ready).\n"
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "import numpy as np\n",
      "\n",
      "def check(name: str, cond: bool):\n",
      "    if not cond:\n",
      "        raise AssertionError(f'Failed: {name}')\n",
      "    print(f'OK: {name}')\n",
      "\n",
      "rng = np.random.default_rng(0)"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 1 — Discrete Random Variables\n",
      "\n",
      "### Task 1.1: Expectation & variance from a PMF\n",
      "Given values x and probabilities p (sum to 1):\n",
      "- implement E[X] and Var(X)\n",
      "\n",
      "# HINT:\n",
      "- E[X] = sum p_i x_i\n",
      "- Var(X) = E[X^2] - (E[X])^2\n",
      "\n",
      "**Explain:** Why is variance not linear, but expectation is?"
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def expectation(x, p):\n",
      "    # TODO\n",
      "    ...\n",
      "\n",
      "def variance(x, p):\n",
      "    # TODO\n",
      "    ...\n",
      "\n",
      "x = np.array([0, 1, 2])\n",
      "p = np.array([0.2, 0.5, 0.3])\n",
      "mu = expectation(x, p)\n",
      "var = variance(x, p)\n",
      "print('E[X]=', mu, 'Var=', var)\n",
      "check('mu', abs(mu - 1.1) < 1e-9)\n",
      "check('var', abs(var - (0.2*0 + 0.5*1 + 0.3*4 - 1.1**2)) < 1e-9)"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 2 — Conditional Probability & Bayes\n",
      "\n",
      "### Task 2.1: Bayes theorem (classic interview)\n",
      "Disease test example:\n",
      "- prevalence P(D)=0.01\n",
      "- sensitivity P(+|D)=0.99\n",
      "- false positive rate P(+|~D)=0.05\n",
      "Compute P(D|+)\n",
      "\n",
      "# HINT:\n",
      "P(D|+) = P(+|D)P(D) / (P(+|D)P(D) + P(+|~D)P(~D))\n",
      "\n",
      "**FAANG gotcha:** base-rate fallacy."
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "P_D = 0.01\n",
      "P_pos_given_D = 0.99\n",
      "P_pos_given_notD = 0.05\n",
      "\n",
      "# TODO\n",
      "P_D_given_pos = ...\n",
      "print('P(D|+)=', P_D_given_pos)\n",
      "check('range', 0 <= P_D_given_pos <= 1)\n",
      "# Should be around 0.166...\n",
      "check('approx', abs(P_D_given_pos - (0.99*0.01)/(0.99*0.01 + 0.05*0.99)) < 1e-12)"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "### Task 2.2: Simulation check (sanity)\n",
      "Simulate N people and estimate P(D|+) empirically.\n",
      "\n",
      "# HINT:\n",
      "- sample disease ~ Bernoulli(P_D)\n",
      "- sample test result conditional on disease\n",
      "\n",
      "**Explain:** Why does simulation converge to the analytic value?"
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "N = 200000\n",
      "# TODO\n",
      "disease = ...\n",
      "test_pos = ...\n",
      "\n",
      "# estimate P(D|+)\n",
      "est = ...\n",
      "print('estimate', est)\n",
      "check('close', abs(est - P_D_given_pos) < 0.01)"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 3 — Continuous Distributions (Normal)\n",
      "\n",
      "### Task 3.1: Standardization (z-score)\n",
      "Given X ~ Normal(mu, sigma^2). Compute standardized Z=(X-mu)/sigma.\n",
      "\n",
      "# HINT:\n",
      "- simulate X and check Z mean ~0, std ~1\n",
      "\n",
      "**ML link:** standardization shows up in preprocessing and SGD stability."
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "mu, sigma = 5.0, 2.0\n",
      "X = rng.normal(mu, sigma, size=200000)\n",
      "# TODO\n",
      "Z = ...\n",
      "print('Z mean', Z.mean(), 'Z std', Z.std())\n",
      "check('mean0', abs(Z.mean()) < 0.02)\n",
      "check('std1', abs(Z.std() - 1.0) < 0.02)"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 4 — Naive Bayes Thinking (Optional Mini)\n",
      "\n",
      "### Task 4.1: Compute log-odds for a toy Naive Bayes\n",
      "Given word likelihoods for spam vs ham, compute posterior odds for a message.\n",
      "\n",
      "# HINT:\n",
      "- work in log space (sum logs)\n",
      "\n",
      "**FAANG gotcha:** multiplying small probabilities underflows; use logs."
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "# Toy params\n",
      "prior_spam = 0.2\n",
      "prior_ham = 0.8\n",
      "P_word_given_spam = {'free': 0.08, 'win': 0.05, 'meeting': 0.001}\n",
      "P_word_given_ham  = {'free': 0.002, 'win': 0.001, 'meeting': 0.03}\n",
      "message = ['free', 'win']\n",
      "\n",
      "# TODO: compute log posterior ratio log P(spam|msg) - log P(ham|msg) up to constant\n",
      "log_ratio = ...\n",
      "print('log_ratio', log_ratio)\n",
      "check('finite', np.isfinite(log_ratio))"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "---\n",
      "## Submission Checklist\n",
      "- All TODOs completed\n",
      "- Checks pass\n",
      "- Explain prompts answered\n"
    ]}
  ]
}
